{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from f110_gym.envs.base_classes import Integrator\n",
    "from collections import Counter,defaultdict\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict,deque\n",
    "import math\n",
    "import cmath\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward:\n",
    "    def __init__(self, min_speed=0.8, max_speed=1.8, map_centers=None):\n",
    "        \"\"\"\n",
    "        Initialize the Reward class.\n",
    "\n",
    "        Parameters:\n",
    "        min_speed (float): Minimum speed.\n",
    "        max_speed (float): Maximum speed.\n",
    "        map_centers (np.ndarray): Array of map center coordinates.\n",
    "        \"\"\"\n",
    "        self.min_speed = min_speed\n",
    "        self.max_speed = max_speed\n",
    "        self.map_centers = map_centers\n",
    "        self.initial_point = np.array([[0, 0]])\n",
    "\n",
    "        # Calculate total track length\n",
    "        self.track_lengths = [np.linalg.norm(self.map_centers[i, :] - self.map_centers[i + 1, :]) for i in range(self.map_centers.shape[0] - 1)]\n",
    "        self.total_track_length = np.sum(self.track_lengths)\n",
    "\n",
    "        self.difference = np.diff(self.map_centers, axis=0)  # Calculate the difference between consecutive centers\n",
    "        self.l2 = np.linalg.norm(self.difference, axis=1)    # Calculate the L2 norm of the difference\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.track_width = 2.2\n",
    "        self.epsilon = 1e-5\n",
    "        self.distance_travelled = 0\n",
    "        self.distance_multiplication_factor = 50\n",
    "        self.distance_scaling_factor = 1.2\n",
    "\n",
    "        self.min_distance = self.distance_multiplication_factor * ((np.exp(-0) / (2 - np.exp(-0))) - 0.7) + self.epsilon\n",
    "        self.max_distance = self.distance_multiplication_factor * ((np.exp(-1) / (2 - np.exp(-1))) - 0.7) + self.epsilon\n",
    "\n",
    "    def reset(self, point):\n",
    "        \"\"\"\n",
    "        Reset the distance travelled and initial point.\n",
    "\n",
    "        Parameters:\n",
    "        point (np.ndarray): Initial point coordinates.\n",
    "        \"\"\"\n",
    "        self.distance_travelled = 0\n",
    "        self.initial_point = point\n",
    "\n",
    "    def distance_reward(self, curr_position, next_position):\n",
    "        \"\"\"\n",
    "        Calculate the distance reward.\n",
    "\n",
    "        Parameters:\n",
    "        curr_position (np.ndarray): Current position coordinates.\n",
    "        next_position (np.ndarray): Next position coordinates.\n",
    "\n",
    "        Returns:\n",
    "        float: Distance reward.\n",
    "        \"\"\"\n",
    "        distance = np.linalg.norm(curr_position - next_position)\n",
    "        self.distance_travelled += distance\n",
    "        return self.distance_travelled / self.total_track_length\n",
    "\n",
    "    def calculate_distance_from_center(self, curr_x, curr_y):\n",
    "        \"\"\"\n",
    "        Calculate the distance from the current position to the track centers.\n",
    "\n",
    "        Parameters:\n",
    "        curr_x (float): Current x-coordinate.\n",
    "        curr_y (float): Current y-coordinate.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Index of the closest center and the distance to it.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(self.map_centers - np.array([curr_x, curr_y]), axis=1)\n",
    "        return np.argmin(distances), distances[np.argmin(distances)]\n",
    "\n",
    "    def centering_reward(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate the centering reward.\n",
    "\n",
    "        Parameters:\n",
    "        x (float): Current x-coordinate.\n",
    "        y (float): Current y-coordinate.\n",
    "\n",
    "        Returns:\n",
    "        float: Centering reward.\n",
    "        \"\"\"\n",
    "        _, distance = self.calculate_distance_from_center(x, y)\n",
    "        d = self.distance_multiplication_factor * ((np.exp(-distance) / (2 - np.exp(-distance))) - 0.6) + self.epsilon\n",
    "\n",
    "        return self.distance_scaling_factor * d / (self.min_distance - self.max_distance)\n",
    "\n",
    "    def calculate_reward(self, curr_state, next_state):\n",
    "        \"\"\"\n",
    "        Calculate the total reward.\n",
    "\n",
    "        Parameters:\n",
    "        curr_state (np.ndarray): Current state coordinates.\n",
    "        next_state (np.ndarray): Next state coordinates.\n",
    "\n",
    "        Returns:\n",
    "        float: Total reward.\n",
    "        \"\"\"\n",
    "        distance_reward = self.distance_reward(curr_state, next_state)\n",
    "        centering_reward = self.centering_reward(next_state[0], next_state[1])\n",
    "        # print(f\"Distance reward: {2*distance_reward}, Centering reward: {centering_reward}\")\n",
    "        return 2*distance_reward + centering_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexSelector:\n",
    "    def __init__(self, num_indices):\n",
    "        \"\"\"\n",
    "        Initialize the IndexSelector class.\n",
    "\n",
    "        Parameters:\n",
    "        num_indices (int): Number of indices.\n",
    "        \"\"\"\n",
    "        self.num_indices = num_indices\n",
    "        self.visited_indices = set()\n",
    "        self.probabilities = np.ones(num_indices) / num_indices\n",
    "\n",
    "    def select_index(self):\n",
    "        \"\"\"\n",
    "        Select an index based on the current probabilities.\n",
    "\n",
    "        Returns:\n",
    "        int: Selected index.\n",
    "        \"\"\"\n",
    "        if len(self.visited_indices) == self.num_indices:\n",
    "            # Reset the probabilities and visited indices\n",
    "            print('Visited all indices, resetting')\n",
    "            self.visited_indices = set()\n",
    "            self.probabilities = np.ones(self.num_indices) / self.num_indices\n",
    "\n",
    "        # Select an index based on the current probabilities\n",
    "        random_idx = np.random.choice(np.arange(self.num_indices), p=self.probabilities)\n",
    "\n",
    "        # Update the probabilities\n",
    "        self.visited_indices.add(random_idx)\n",
    "        if len(self.visited_indices) < self.num_indices:\n",
    "            self.probabilities[random_idx] = 0\n",
    "            remaining_prob = 1 - np.sum(self.probabilities)\n",
    "            self.probabilities[self.probabilities > 0] += remaining_prob / np.sum(self.probabilities > 0)\n",
    "\n",
    "        return random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Tenth_navigation:\n",
    "    \"\"\"\n",
    "    A class to navigate the F1Tenth environment using SARSA(Î») and Expected SARSA algorithms.\n",
    "\n",
    "    Attributes:\n",
    "        env (gym.Env): The gym environment.\n",
    "        sx, sy, stheta (float): Starting x, y coordinates and orientation.\n",
    "        save_path (str): Path to save the weights.\n",
    "        track_name (str): Name of the track.\n",
    "        num_agents (int): Number of agents.\n",
    "        map_path (str): Path to the map.\n",
    "        map_ext (str): Extension of the map file.\n",
    "        map_centers (np.ndarray): Array of map center coordinates.\n",
    "        index_selector (IndexSelector): Index selector for resetting positions.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "        num_beams (int): Number of LiDAR beams.\n",
    "        n_features (int): Number of binary features.\n",
    "        binary_powers (np.ndarray): Powers of 2 for binary conversion.\n",
    "        angle (int): Angle for LiDAR.\n",
    "        num_states (int): Number of states.\n",
    "        num_actions (int): Number of actions.\n",
    "        angles_deg (np.ndarray): Array of angles in degrees.\n",
    "        angles_rad (np.ndarray): Array of angles in radians.\n",
    "        min_speed, max_speed (float): Minimum and maximum speed.\n",
    "        mu, sig (float): Mean and standard deviation for Gaussian speed distribution.\n",
    "        action_space (dict): Action space mapping.\n",
    "        weights (np.ndarray): Q-table weights.\n",
    "        ET (np.ndarray): Eligibility trace.\n",
    "        IS (np.ndarray): Instructive signal.\n",
    "        discount_factor (float): Discount factor for SARSA.\n",
    "        learning_rate (float): Learning rate for SARSA.\n",
    "        decay_rate (float): Decay rate for eligibility trace.\n",
    "        IS_decay_rate (float): Decay rate for instructive signal.\n",
    "        epsilon_treshold (float): Epsilon threshold for epsilon-greedy policy.\n",
    "        numerical_stability (float): Numerical stability constant.\n",
    "        reward_class (Reward): Reward calculation class.\n",
    "        reward, cumulative_reward (float): Reward values.\n",
    "        episode_reward (list): List of episode rewards.\n",
    "        curr_state (int): Current state index.\n",
    "        curr_action_index (int): Current action index.\n",
    "        collision_count (int): Collision count.\n",
    "        distance_threshold (float): Distance threshold for reset.\n",
    "        state_counter (np.ndarray): Counter for state visits.\n",
    "        reset_position (np.ndarray): Reset position coordinates.\n",
    "        current_lap (int): Current lap count.\n",
    "        visited_states (set): Set of visited states.\n",
    "        visited_indices (set): Set of visited indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gym_env_code='f110_gym:f110-v0', num_agents=1, map_path='./f1tenth_racetracks/Austin/Austin_map', map_ext='.png', sx=0., sy=0., stheta=0., map_centers_file=None, save_path=None, track_name=None, inference=None):\n",
    "        \"\"\"\n",
    "        Initialize the F1Tenth_navigation class.\n",
    "\n",
    "        Parameters:\n",
    "            gym_env_code (str): Gym environment code.\n",
    "            num_agents (int): Number of agents.\n",
    "            map_path (str): Path to the map.\n",
    "            map_ext (str): Extension of the map file.\n",
    "            sx, sy, stheta (float): Starting x, y coordinates and orientation.\n",
    "            map_centers_file (str): Path to the map centers file.\n",
    "            save_path (str): Path to save the weights.\n",
    "            track_name (str): Name of the track.\n",
    "            inference (str): Path to the inference weights file.\n",
    "        \"\"\"\n",
    "        self.env = gym.make(gym_env_code, map=map_path, map_ext=map_ext, num_agents=num_agents, timestep=0.01, integrator=Integrator.RK4)\n",
    "        self.env.add_render_callback(self.render_callback)\n",
    "        self.sx, self.sy, self.stheta = sx, sy, stheta\n",
    "        self.save_path = save_path\n",
    "        self.track_name = track_name\n",
    "        self.num_agents = num_agents\n",
    "        self.map_path = map_path\n",
    "        self.map_ext = map_ext\n",
    "        self.map_centers_file = pd.read_csv(map_centers_file)\n",
    "        self.map_centers_file.columns = ['x', 'y', 'w_r', 'w_l']\n",
    "        self.map_centers_file.index = self.map_centers_file.index.astype(int)\n",
    "        self.map_centers = self.map_centers_file.values[:, :2]\n",
    "\n",
    "        self.index_selector = IndexSelector(self.map_centers.shape[0])\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        self.random_seed = 42\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "        self.num_beams = 1080\n",
    "        self.n_features = 10\n",
    "        self.binary_powers = np.array([2 ** i for i in range(self.n_features)])\n",
    "        self.angle = 270\n",
    "\n",
    "        self.num_states = 2 ** self.n_features\n",
    "        self.num_actions = 51\n",
    "        \n",
    "\n",
    "        self.angles_deg = np.linspace(-self.angle // 2, self.angle // 2, self.num_actions)[::-1]\n",
    "        self.angles_rad = self.convert_deg_to_rad(self.angles_deg)\n",
    "\n",
    "        self.min_speed = 0.5\n",
    "        self.max_speed = 1.8\n",
    "        self.mu = self.num_actions // 2\n",
    "        self.sig = self.num_actions // 4\n",
    "\n",
    "        # self.speeds = np.linspace(self.min_speed, self.max_speed, self.num_actions)\n",
    "        self.action_space = self.generate_action_space(self.angles_rad)\n",
    "\n",
    "        if inference is not None:\n",
    "            self.weights = np.load(inference)\n",
    "            print(f'Loaded already trained weights')\n",
    "            self.collision_count = int(inference.split('_')[1].split('.')[0])\n",
    "        else:\n",
    "            self.weights = np.zeros((self.num_states, self.num_actions))\n",
    "            self.collision_count = 0\n",
    "        \n",
    "        self.ET_IS = np.zeros((self.num_states, self.num_actions))\n",
    "\n",
    "        # self.ET = np.zeros((self.num_states, 1))\n",
    "        # self.IS = np.zeros((1, self.num_actions))\n",
    "\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.1\n",
    "        self.decay_rate = 0.9\n",
    "        self.IS_decay_rate = 0.7\n",
    "        self.epsilon_decay_rate=0.99997\n",
    "\n",
    "        self.epsilon_treshold = 0.25\n",
    "        self.numerical_stability = 1e-5\n",
    "\n",
    "        self.reward_class = Reward(min_speed=self.min_speed, max_speed=self.max_speed, map_centers=self.map_centers)\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.episode_reward = []\n",
    "        self.multi_episode_reward=[]\n",
    "\n",
    "        self.curr_state = None\n",
    "        self.curr_action_index = None\n",
    "\n",
    "        self.distance_threshold = 0.5\n",
    "        self.state_counter = np.zeros(self.num_states)\n",
    "        self.reset_position = np.array([[self.sx, self.sy]])\n",
    "\n",
    "        self.current_lap = 1\n",
    "        self.visited_states = set()\n",
    "        self.visited_indices = set()\n",
    "\n",
    "    def render_callback(self, env_renderer):\n",
    "        \"\"\"\n",
    "        Render callback function to update the camera to follow the car.\n",
    "\n",
    "        Parameters:\n",
    "            env_renderer (EnvRenderer): The environment renderer.\n",
    "        \"\"\"\n",
    "        e = env_renderer\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "\n",
    "    def convert_deg_to_rad(self, array):\n",
    "        \"\"\"\n",
    "        Convert an array of angles from degrees to radians.\n",
    "\n",
    "        Parameters:\n",
    "            array (np.ndarray): Array of angles in degrees.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of angles in radians.\n",
    "        \"\"\"\n",
    "        return array * np.pi / 180\n",
    "\n",
    "    def gaussian_speed(self, location):\n",
    "        \"\"\"\n",
    "        Calculate the Gaussian speed for a given location.\n",
    "\n",
    "        Parameters:\n",
    "            location (int): Location index.\n",
    "\n",
    "        Returns:\n",
    "            float: Gaussian speed.\n",
    "        \"\"\"\n",
    "        speed = (1 / (self.sig * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((location - self.mu) / self.sig) ** 2)\n",
    "        scale_factor = self.max_speed / (1 / (self.sig * np.sqrt(2 * np.pi)))\n",
    "        scaled_value = speed * scale_factor\n",
    "        return np.round(np.clip(scaled_value, self.min_speed, self.max_speed), 4)\n",
    "\n",
    "    def generate_action_space(self, angles):\n",
    "        \"\"\"\n",
    "        Generate the action space mapping.\n",
    "\n",
    "        Parameters:\n",
    "            angles (np.ndarray): Array of angles in radians.\n",
    "\n",
    "        Returns:\n",
    "            dict: Action space mapping.\n",
    "        \"\"\"\n",
    "        action_space = {}\n",
    "        for idx, angle in enumerate(angles):\n",
    "            # action_space[idx] = (angle, self.gaussian_speed(idx))\n",
    "            action_space[idx] = (angle, 1.0)\n",
    "        return action_space\n",
    "\n",
    "    def lidar_to_binary_features(self, lidar_input, n_features=10, n_sectors=36, threshold_percentile=40):\n",
    "        \"\"\"\n",
    "        Convert LiDAR scan of 1080 values to a binary representation of 10 bits.\n",
    "\n",
    "        Parameters:\n",
    "            lidar_input (np.ndarray): LiDAR input array of shape (1080,).\n",
    "            n_features (int): Number of binary features to generate (default: 10).\n",
    "            n_sectors (int): Number of sectors to divide scan into (default: 36).\n",
    "            threshold_percentile (int): Percentile for thresholding (default: 50).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Binary features array of shape (n_features,).\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(self.random_seed)\n",
    "        lidar_input = np.asarray(lidar_input, dtype=np.float32)\n",
    "\n",
    "        sector_size = lidar_input.shape[0] // n_sectors\n",
    "        sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "\n",
    "        sector_features = np.concatenate([\n",
    "            np.mean(sectors, axis=1),\n",
    "            np.std(sectors, axis=1),\n",
    "            np.max(sectors, axis=1),\n",
    "            np.min(sectors, axis=1),\n",
    "            np.median(sectors, axis=1),\n",
    "            np.percentile(sectors, 75, axis=1)\n",
    "        ])\n",
    "\n",
    "        gradients = []\n",
    "        for scale in [1, 2, 3]:\n",
    "            rolled = np.roll(sector_features, scale)\n",
    "            gradient = sector_features - rolled\n",
    "            gradients.append(gradient)\n",
    "        gradient_features = np.concatenate(gradients)\n",
    "\n",
    "        combined_features = np.concatenate([sector_features, gradient_features, np.abs(gradient_features)])\n",
    "        combined_features = (combined_features - np.mean(combined_features)) / (np.std(combined_features) + 1e-8)\n",
    "\n",
    "        input_dim = combined_features.shape[0]\n",
    "        projection_matrix = rng.normal(0, 1, (input_dim, n_features))\n",
    "        projection_matrix /= np.linalg.norm(projection_matrix, axis=0) + 1e-8\n",
    "\n",
    "        projected_features = np.dot(combined_features, projection_matrix)\n",
    "        thresholds = np.percentile(projected_features, threshold_percentile, axis=0)\n",
    "\n",
    "        temperature = 0.7\n",
    "        z_scores = (projected_features - thresholds) / temperature\n",
    "        probabilities = stats.norm.cdf(z_scores)\n",
    "\n",
    "        random_matrix = rng.uniform(0, 1, probabilities.shape)\n",
    "        binary_features = (probabilities >= random_matrix).astype(np.int8)\n",
    "\n",
    "        return binary_features\n",
    "\n",
    "    # def lidar_to_binary_features(self,lidar_input, scaler,binary_threshold=0.0,n_sectors=12):\n",
    "    #     \"\"\"\n",
    "    #     Convert LiDAR input into a binary representation using statistical features.\n",
    "\n",
    "    #     Parameters:\n",
    "    #     - lidar_input: 1D array of LiDAR values.\n",
    "    #     - n_sectors: Number of sectors to divide the input into.\n",
    "\n",
    "    #     Returns:\n",
    "    #     - binary_code: 1D array of binary representation.\n",
    "    #     \"\"\"\n",
    "    #     lidar_input = np.asarray(lidar_input, dtype=np.float32)\n",
    "    #     sector_size = lidar_input.shape[0] // n_sectors\n",
    "    #     sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "    #     sector_features = np.vstack(\n",
    "    #         [   np.mean(sectors, axis=1),\n",
    "    #             np.std(sectors, axis=1),\n",
    "    #             np.max(sectors, axis=1),\n",
    "    #             np.min(sectors, axis=1),\n",
    "    #             np.median(sectors, axis=1),\n",
    "    #             np.percentile(sectors, 75, axis=1)\n",
    "    #         ])\n",
    "    #     scaled_features = scaler.partial_fit(sector_features.T).transform(sector_features.T).T\n",
    "    #     binary_features = (scaled_features > binary_threshold).astype(int)\n",
    "\n",
    "    #     binary_code = (binary_features.sum(axis=0) > 0).astype(int)  \n",
    "    #     return np.array(binary_code).reshape(1,-1)\n",
    "\n",
    "    def vectorized_10bit_lsh(self,x, random_state):\n",
    "            \"\"\"\n",
    "            Vectorized LSH bit generation.\n",
    "            \"\"\"\n",
    "            # Projection matrix generation\n",
    "            input_dim = len(x)\n",
    "            n_projections = max(20, input_dim)\n",
    "            \n",
    "            # Vectorized Cauchy projection\n",
    "            projection_matrix = random_state.standard_cauchy((input_dim, n_projections))\n",
    "            \n",
    "            # Normalize projection matrix columns\n",
    "            projection_matrix /= np.linalg.norm(projection_matrix, axis=0, keepdims=True)\n",
    "            \n",
    "            # Vectorized projection\n",
    "            projected = x @ projection_matrix\n",
    "            \n",
    "            # Compute thresholds\n",
    "            thresholds = np.median(projected, axis=0)\n",
    "            \n",
    "            # Vectorized bit generation\n",
    "            bit_matrix = (projected >= thresholds).astype(np.int8)\n",
    "            \n",
    "            # Vectorized 10-bit generation using rolling XOR\n",
    "            # Create a sliding window of projections\n",
    "            sliding_window = np.lib.stride_tricks.sliding_window_view(bit_matrix.T, 2)\n",
    "            \n",
    "            # Compute XOR across sliding windows\n",
    "            xor_bits = np.bitwise_xor(sliding_window[..., 0], sliding_window[..., 1])\n",
    "            \n",
    "            # Take first 10 bits\n",
    "            return xor_bits[:self.n_features]\n",
    "    \n",
    "    # def lidar_to_binary_features(self, lidar_input, n_features=10, n_sectors=36):\n",
    "    #     \"\"\"\n",
    "    #     Convert LiDAR scan to binary features using Locality-Sensitive Hashing.\n",
    "        \n",
    "    #     Parameters:\n",
    "    #         lidar_input (np.ndarray): LiDAR input array\n",
    "    #         n_features (int): Number of binary features to generate\n",
    "    #         n_sectors (int): Number of sectors to divide scan into\n",
    "        \n",
    "    #     Returns:\n",
    "    #         np.ndarray: Binary features array\n",
    "    #     \"\"\"\n",
    "    #     lidar_input = np.asarray(lidar_input, dtype=np.float32)\n",
    "        \n",
    "    #     # Reshape into sectors\n",
    "    #     sector_size = lidar_input.shape[0] // n_sectors\n",
    "    #     sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "        \n",
    "    #     # Vectorized statistical features\n",
    "    #     sector_stats = np.column_stack([\n",
    "    #         np.mean(sectors, axis=1),\n",
    "    #         np.std(sectors, axis=1),\n",
    "    #         np.max(sectors, axis=1),\n",
    "    #         np.min(sectors, axis=1),\n",
    "    #         np.median(sectors, axis=1),\n",
    "    #         np.percentile(sectors, 75, axis=1)\n",
    "    #         ])\n",
    "        \n",
    "    #     # Vectorized gradient computation\n",
    "    #     scales = np.array([1, 2, 3])\n",
    "    #     rolled_features = np.stack([\n",
    "    #         np.roll(sector_stats, shift) for shift in scales\n",
    "    #     ])\n",
    "        \n",
    "    #     # Compute gradient variations\n",
    "    #     gradient_ops = np.stack([\n",
    "    #         sector_stats - rolled_features,  # difference\n",
    "    #         np.abs(sector_stats - rolled_features),  # absolute difference\n",
    "    #         np.square(sector_stats - rolled_features)  # squared difference\n",
    "    #     ])\n",
    "        \n",
    "    #     # Flatten and combine features\n",
    "    #     combined_features = np.concatenate([\n",
    "    #         sector_stats.ravel(),\n",
    "    #         gradient_ops.ravel()\n",
    "    #     ])\n",
    "        \n",
    "    #     # Normalized features\n",
    "    #     features = (combined_features - np.mean(combined_features)) / (np.std(combined_features) + 1e-8)\n",
    "    #     rng = np.random.RandomState(self.random_seed)\n",
    "    #     # Generate binary features\n",
    "    #     binary_features = self.vectorized_10bit_lsh(features, rng)\n",
    "    #     return np.array(binary_features).reshape(1,-1)\n",
    "    \n",
    "    def get_state(self, feature):\n",
    "        \"\"\"\n",
    "        Get the state index from the binary feature vector.\n",
    "\n",
    "        Parameters:\n",
    "            feature (np.ndarray): Binary feature vector.\n",
    "\n",
    "        Returns:\n",
    "            int: State index.\n",
    "        \"\"\"\n",
    "        return np.dot(feature[0], self.binary_powers)\n",
    "\n",
    "    def select_action(self, state_idx):\n",
    "        \"\"\"\n",
    "        Select an action based on the epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "            state_idx (int): State index.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Action index and action (steering angle, speed).\n",
    "        \"\"\"\n",
    "        random_number = np.random.rand()\n",
    "        action_idx = None\n",
    "\n",
    "        if random_number <= self.epsilon_treshold:\n",
    "            action_idx = np.random.choice(list(self.action_space.keys()))\n",
    "        else:\n",
    "            max_value = np.max(self.weights[state_idx])\n",
    "            max_indices = np.where(np.abs(self.weights[state_idx] == max_value))[0]\n",
    "            action_idx = np.random.choice(max_indices)\n",
    "\n",
    "        self.epsilon_treshold *= self.epsilon_decay_rate\n",
    "        return action_idx, self.action_space[action_idx]\n",
    "\n",
    "    def inference_action(self, state_idx):\n",
    "        \"\"\"\n",
    "        Select the action with the highest Q-value for inference.\n",
    "\n",
    "        Parameters:\n",
    "            state_idx (int): State index.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Action index and action (steering angle, speed).\n",
    "        \"\"\"\n",
    "        action_idx = np.argmax(self.weights[state_idx])\n",
    "        return action_idx, self.action_space[action_idx]\n",
    "\n",
    "    def normalize_weights(self,values, neg_lower_bound=-3, neg_upper_bound=-0.1, pos_lower_bound=0.1, pos_upper_bound=3):\n",
    "        values = np.array(values)\n",
    "    \n",
    "        pos_mask = values > 0\n",
    "        neg_mask = values < 0\n",
    "        \n",
    "        pos_values = values[pos_mask]\n",
    "        neg_values = values[neg_mask]\n",
    "        \n",
    "        normalized_values = np.zeros_like(values, dtype=float)\n",
    "        \n",
    "        if pos_values.size > 0:\n",
    "            pos_min = pos_values.min()\n",
    "            pos_max = pos_values.max()\n",
    "            if pos_min != pos_max:\n",
    "                pos_normalized = (pos_values - pos_min) / (pos_max - pos_min)\n",
    "                pos_scaled = pos_lower_bound + (pos_upper_bound - pos_lower_bound) * pos_normalized\n",
    "            else:\n",
    "                pos_scaled = np.full(pos_values.shape, pos_lower_bound)\n",
    "            normalized_values[pos_mask] = pos_scaled\n",
    "        \n",
    "        if neg_values.size > 0:\n",
    "            neg_min = neg_values.min()\n",
    "            neg_max = neg_values.max()\n",
    "            if neg_min != neg_max:\n",
    "                neg_normalized = (neg_values - neg_min) / (neg_max - neg_min)\n",
    "                neg_scaled = neg_lower_bound + (neg_upper_bound - neg_lower_bound) * neg_normalized\n",
    "            else:\n",
    "                neg_scaled = np.full(neg_values.shape, neg_upper_bound)\n",
    "            normalized_values[neg_mask] = neg_scaled\n",
    "        \n",
    "        return np.round(normalized_values,4)\n",
    "\n",
    "    def expected_sarsa_weight_update(self, next_state, reward):\n",
    "        \"\"\"\n",
    "        Update the weights using the Expected SARSA algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            next_state (int): Next state index.\n",
    "            reward (float): Reward received.\n",
    "        \"\"\"\n",
    "        # self.ET[self.curr_state, :] = 1\n",
    "        # self.IS[:, self.curr_action_index] = 1\n",
    "        self.ET_IS[self.curr_state, self.curr_action_index] = 1\n",
    "\n",
    "        q_values = self.weights[next_state, :]\n",
    "        q_max = np.max(q_values)\n",
    "        greedy_actions = np.sum(q_values == q_max)\n",
    "\n",
    "        non_greedy_action_probability = self.epsilon_treshold / self.num_actions\n",
    "        greedy_action_probability = ((1 - self.epsilon_treshold) / greedy_actions) + non_greedy_action_probability\n",
    "\n",
    "        greedy_mask = (q_values == q_max)\n",
    "        non_greedy_mask = ~greedy_mask\n",
    "\n",
    "        expected_q = np.sum(q_values[greedy_mask] * greedy_action_probability) + np.sum(q_values[non_greedy_mask] * non_greedy_action_probability)\n",
    "\n",
    "        delta = reward + self.discount_factor * expected_q - self.weights[self.curr_state, self.curr_action_index]\n",
    "\n",
    "        # self.weights += self.learning_rate * delta * (self.ET @ self.IS)\n",
    "        self.weights += self.learning_rate * delta * self.ET_IS\n",
    "        self.weights = self.normalize_weights(self.weights)\n",
    "        self.decay_ET_IS()\n",
    "\n",
    "    def sarsa_lambda_weight_update(self, next_state, reward):\n",
    "        \"\"\"\n",
    "        Update the weights using the SARSA(Î») algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            next_state (int): Next state index.\n",
    "            reward (float): Reward received.\n",
    "        \"\"\"\n",
    "        # self.ET[self.curr_state, :] = 1\n",
    "        # self.IS[:, self.curr_action_index] = 1\n",
    "        self.ET_IS[self.curr_state, self.curr_action_index] = 1\n",
    "\n",
    "        next_action_index, _ = self.select_action(next_state)\n",
    "        delta = reward + self.discount_factor * self.weights[next_state, next_action_index] - self.weights[self.curr_state, self.curr_action_index]\n",
    "\n",
    "        # self.weights += self.learning_rate * delta * (self.ET @ self.IS)\n",
    "        self.weights += self.learning_rate * delta * self.ET_IS\n",
    "        self.weights = self.normalize_weights(self.weights)\n",
    "        self.decay_ET_IS()\n",
    "\n",
    "    def decay_ET_IS(self):\n",
    "        \"\"\"\n",
    "        Decay the eligibility trace and instructive signal.\n",
    "        \"\"\"\n",
    "        # self.ET *= self.discount_factor * self.decay_rate\n",
    "        # self.IS *= self.discount_factor * self.IS_decay_rate\n",
    "        self.ET_IS *= self.discount_factor * self.decay_rate\n",
    "\n",
    "        # self.ET = np.round(self.ET, 4)\n",
    "        # self.IS = np.round(self.IS, 4)\n",
    "        # self.ET_IS = np.round(self.ET_IS, 4)\n",
    "\n",
    "    def reset_ET_IS(self):\n",
    "        \"\"\"\n",
    "        Reset the eligibility trace and instructive signal.\n",
    "        \"\"\"\n",
    "        # self.ET.fill(0)\n",
    "        # self.IS.fill(0)\n",
    "        self.ET_IS.fill(0)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using the Expected SARSA algorithm.\n",
    "        \"\"\"\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.sx, self.sy, self.stheta]]))\n",
    "        lidar = obs['scans'][0]\n",
    "        # down_sampled = self.lidar_to_binary_features(lidar,self.scaler,n_sectors=self.n_features).reshape(1, -1)\n",
    "        down_sampled = self.lidar_to_binary_features(lidar).reshape(1,-1)\n",
    "        self.curr_state = self.get_state(down_sampled)\n",
    "        episode_reward = 0\n",
    "        self.reset_position = np.array([[self.sx, self.sy]])\n",
    "        self.reward_class.reset(self.reset_position)\n",
    "        start_time=time.time()\n",
    "        while True:\n",
    "            if self.curr_state not in self.visited_states:\n",
    "                self.visited_states.add(self.curr_state)\n",
    "\n",
    "            print(f'Downsampled is {down_sampled} and curr state is {self.curr_state}')\n",
    "            self.curr_action_index, (steering_angle, speed) = self.select_action(self.curr_state)\n",
    "            print(f'Action index is {self.curr_action_index} and action is {steering_angle, speed}')\n",
    "            curr_x = obs['poses_x'][0]\n",
    "            curr_y = obs['poses_y'][0]\n",
    "            obs, reward, done, info = self.env.step(np.array([[steering_angle, speed]]))\n",
    "            lidar = obs['scans'][0]\n",
    "            # down_sampled = self.lidar_to_binary_features(lidar,self.scaler,n_sectors=self.n_features).reshape(1, -1)\n",
    "            down_sampled = self.lidar_to_binary_features(lidar).reshape(1,-1)\n",
    "            next_state = self.get_state(down_sampled)\n",
    "\n",
    "            if done:\n",
    "                self.reward = -100\n",
    "            else:\n",
    "                self.reward = self.reward_class.calculate_reward(np.array([curr_x, curr_y]), np.array([obs['poses_x'][0], obs['poses_y'][0]]))\n",
    "\n",
    "            self.cumulative_reward += self.reward\n",
    "            episode_reward += self.reward\n",
    "\n",
    "            self.expected_sarsa_weight_update(next_state, self.reward)\n",
    "            # self.sarsa_lambda_weight_update(next_state, self.reward)\n",
    "            self.curr_state = next_state\n",
    "\n",
    "            if done:\n",
    "                random_idx = self.index_selector.select_index()\n",
    "                n_x, n_y = self.map_centers[random_idx]\n",
    "                delta_x, delta_y = np.random.uniform(-0.75, 0.75), np.random.uniform(-0.2, 0.2)\n",
    "                n_theta = np.random.choice(self.angles_rad)\n",
    "                obs, step_reward, done, info = self.env.reset(np.array([[n_x + delta_x, n_y + delta_y, n_theta]]))\n",
    "                lidar = obs['scans'][0]\n",
    "                # down_sampled = self.lidar_to_binary_features(lidar,self.scaler,n_sectors=self.n_features).reshape(1, -1)\n",
    "                down_sampled = self.lidar_to_binary_features(lidar).reshape(1,-1)\n",
    "                self.curr_state = self.get_state(down_sampled)\n",
    "\n",
    "                self.reset_position = np.array([[n_x + delta_x, n_y + delta_y]])\n",
    "                self.reward_class.reset(self.reset_position)\n",
    "                self.reset_ET_IS()\n",
    "                self.episode_reward.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "                # print(f'Cumulative is {self.cumulative_reward} and normal is {self.episode_reward}')\n",
    "                if (self.collision_count + 1) % 1000 == 0:\n",
    "                    end_time=time.time()\n",
    "                    self.multi_episode_reward.append(sum(self.episode_reward))\n",
    "                    print(f'Episode:{self.collision_count + 1}, Time:{end_time-start_time} reward:{sum(self.episode_reward)}, Cumulative Reward:{self.cumulative_reward}, observed states:{len(self.visited_states)}')\n",
    "                    self.episode_reward = []\n",
    "                    start_time=end_time\n",
    "                    if not os.path.exists(os.path.join(self.save_path, self.track_name)):\n",
    "                        os.mkdir(os.path.join(self.save_path, self.track_name))\n",
    "                    np.save(os.path.join(self.save_path, self.track_name, f'rewards.npy'), np.array(self.multi_episode_reward))\n",
    "                    \n",
    "                if (self.collision_count + 1) % 5000 == 0:\n",
    "                    if not os.path.exists(os.path.join(self.save_path, self.track_name)):\n",
    "                        os.mkdir(os.path.join(self.save_path, self.track_name))\n",
    "                    np.save(os.path.join(self.save_path, self.track_name, f'weights_{self.collision_count + 1}.npy'), self.weights)\n",
    "                    print(f'File saved')\n",
    "                self.collision_count += 1\n",
    "\n",
    "            self.env.render(mode='human')\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Inference the agent using the Expected SARSA algorithm.\n",
    "        \"\"\"\n",
    "        obs, reward, done, info = self.env.reset(np.array([[self.sx, self.sy, self.stheta]]))\n",
    "        lidar = obs['scans'][0]\n",
    "        # down_sampled = self.lidar_to_binary_features(lidar,self.scaler,n_sectors=self.n_features).reshape(1, -1)\n",
    "        down_sampled = self.lidar_to_binary_features(lidar).reshape(1,-1)\n",
    "        self.curr_state = self.get_state(down_sampled)\n",
    "\n",
    "        while not done:\n",
    "            _, (steering_angle, speed) = self.inference_action(self.curr_state)\n",
    "            obs, reward, done, info = self.env.step(np.array([[steering_angle, speed]]))\n",
    "            lidar = obs['scans'][0]\n",
    "            # down_sampled = self.lidar_to_binary_features(lidar,self.scaler,n_sectors=self.n_features).reshape(1, -1)\n",
    "            down_sampled = self.lidar_to_binary_features(lidar).reshape(1,-1)\n",
    "            self.curr_state = self.get_state(down_sampled)\n",
    "            \n",
    "            if done:\n",
    "                random_idx = self.index_selector.select_index()\n",
    "                n_x, n_y = self.map_centers[random_idx]\n",
    "                delta_x, delta_y = np.random.uniform(-0.75, 0.75), np.random.uniform(-0.2, 0.2)\n",
    "                n_theta = np.random.choice(self.angles_rad)\n",
    "                obs, step_reward, done, info = self.env.reset(np.array([[n_x + delta_x, n_y + delta_y, n_theta]]))\n",
    "                lidar = obs['scans'][0]\n",
    "                # down_sampled = self.lidar_to_binary_features(lidar,self.scaler,n_sectors=self.n_features).reshape(1, -1)\n",
    "                down_sampled = self.lidar_to_binary_features(lidar).reshape(1,-1)\n",
    "                self.curr_state = self.get_state(down_sampled)\n",
    "\n",
    "                self.reset_position = np.array([[n_x + delta_x, n_y + delta_y]])\n",
    "                self.reward_class.reset(self.reset_position)\n",
    "                self.reset_ET_IS()\n",
    "                self.collision_count+=1\n",
    "\n",
    "            self.env.render(mode='human')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/workspaces/F1_tenth_training/f1tenth_gym_ros/f1tenth_racetracks'\n",
    "path = './f1tenth_racetracks'\n",
    "all_map_paths=[]\n",
    "map_centers = []\n",
    "map_names = []\n",
    "track_lengths=[]\n",
    "for folder in os.listdir(path):\n",
    "    if folder not in ['README.md','.gitignore','convert.py','LICENSE','rename.py','.git']:\n",
    "        folder_name=folder\n",
    "        file_name=folder_name.replace(' ','')+'_map'\n",
    "        map_center = folder_name.replace(' ','')+'_centerline.csv'\n",
    "        track_lengths.append(len(pd.read_csv(f'{path}/{folder_name}/{map_center}')))\n",
    "        map_names.append(folder_name)\n",
    "        all_map_paths.append(f'{path}/{folder_name}/{file_name}')\n",
    "        map_centers.append(f'{path}/{folder_name}/{map_center}')\n",
    "\n",
    "list(zip(map_names,track_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global num_agents,map_path,map_ext,sx,sy,stheta\n",
    "num_agents = 1\n",
    "map_ext = '.png'\n",
    "sx = 0.\n",
    "sy = 0.\n",
    "stheta = 1.0\n",
    "map_path = all_map_paths[0]\n",
    "map_center = map_centers[0]\n",
    "map_name = map_names[0]\n",
    "save_path = './Weights/'\n",
    "inference_file = None\n",
    "# inference_file = './Weights/Montreal/weights_55000.npy'\n",
    "simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_center,save_path=save_path,track_name=map_name,inference=inference_file)\n",
    "simulator.train()\n",
    "# simulator.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save(f'{save_path}{map_name}/weights_4000.npy',simulator.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulator.multi_episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
