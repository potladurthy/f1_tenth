{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from f110_gym.envs.base_classes import Integrator\n",
    "from collections import Counter,defaultdict\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict,deque\n",
    "import math\n",
    "import cmath\n",
    "import scipy.stats as stats\n",
    "import time\n",
    "from sklearn.decomposition import PCA\n",
    "from functools import reduce\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward:\n",
    "    def __init__(self, min_speed=0.8, max_speed=1.8, map_centers=None):\n",
    "        \"\"\"\n",
    "        Initialize the Reward class.\n",
    "\n",
    "        Parameters:\n",
    "        min_speed (float): Minimum speed.\n",
    "        max_speed (float): Maximum speed.\n",
    "        map_centers (np.ndarray): Array of map center coordinates.\n",
    "        \"\"\"\n",
    "        self.min_speed = min_speed\n",
    "        self.max_speed = max_speed\n",
    "        self.map_centers = map_centers\n",
    "        self.initial_point = np.array([[0, 0]])\n",
    "\n",
    "        # Calculate total track length\n",
    "        self.track_lengths = [np.linalg.norm(self.map_centers[i, :] - self.map_centers[i + 1, :]) for i in range(self.map_centers.shape[0] - 1)]\n",
    "        self.total_track_length = np.sum(self.track_lengths)\n",
    "\n",
    "        self.difference = np.diff(self.map_centers, axis=0)  # Calculate the difference between consecutive centers\n",
    "        self.l2 = np.linalg.norm(self.difference, axis=1)    # Calculate the L2 norm of the difference\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.track_width = 2.2\n",
    "        self.epsilon = 1e-5\n",
    "        self.distance_travelled = 0\n",
    "        self.distance_multiplication_factor = 50\n",
    "        self.distance_scaling_factor = 1.2\n",
    "\n",
    "        self.min_distance = self.distance_multiplication_factor * ((np.exp(-0) / (2 - np.exp(-0))) - 0.7) + self.epsilon\n",
    "        self.max_distance = self.distance_multiplication_factor * ((np.exp(-1) / (2 - np.exp(-1))) - 0.7) + self.epsilon\n",
    "\n",
    "    def reset(self, point):\n",
    "        \"\"\"\n",
    "        Reset the distance travelled and initial point.\n",
    "\n",
    "        Parameters:\n",
    "        point (np.ndarray): Initial point coordinates.\n",
    "        \"\"\"\n",
    "        self.distance_travelled = 0\n",
    "        self.initial_point = point\n",
    "\n",
    "    def distance_reward(self, curr_position, next_position):\n",
    "        \"\"\"\n",
    "        Calculate the distance reward.\n",
    "\n",
    "        Parameters:\n",
    "        curr_position (np.ndarray): Current position coordinates.\n",
    "        next_position (np.ndarray): Next position coordinates.\n",
    "\n",
    "        Returns:\n",
    "        float: Distance reward.\n",
    "        \"\"\"\n",
    "        distance = np.linalg.norm(curr_position - next_position)\n",
    "        self.distance_travelled += distance\n",
    "        return self.distance_travelled / self.total_track_length\n",
    "\n",
    "    def calculate_distance_from_center(self, curr_x, curr_y):\n",
    "        \"\"\"\n",
    "        Calculate the distance from the current position to the track centers.\n",
    "\n",
    "        Parameters:\n",
    "        curr_x (float): Current x-coordinate.\n",
    "        curr_y (float): Current y-coordinate.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Index of the closest center and the distance to it.\n",
    "        \"\"\"\n",
    "        distances = np.linalg.norm(self.map_centers - np.array([curr_x, curr_y]), axis=1)\n",
    "        return np.argmin(distances), distances[np.argmin(distances)]\n",
    "\n",
    "    def centering_reward(self, x, y):\n",
    "        \"\"\"\n",
    "        Calculate the centering reward.\n",
    "\n",
    "        Parameters:\n",
    "        x (float): Current x-coordinate.\n",
    "        y (float): Current y-coordinate.\n",
    "\n",
    "        Returns:\n",
    "        float: Centering reward.\n",
    "        \"\"\"\n",
    "        _, distance = self.calculate_distance_from_center(x, y)\n",
    "        d = self.distance_multiplication_factor * ((np.exp(-distance) / (2 - np.exp(-distance))) - 0.6) + self.epsilon\n",
    "\n",
    "        return self.distance_scaling_factor * d / (self.min_distance - self.max_distance)\n",
    "\n",
    "    def calculate_reward(self, curr_state, next_state):\n",
    "        \"\"\"\n",
    "        Calculate the total reward.\n",
    "\n",
    "        Parameters:\n",
    "        curr_state (np.ndarray): Current state coordinates.\n",
    "        next_state (np.ndarray): Next state coordinates.\n",
    "\n",
    "        Returns:\n",
    "        float: Total reward.\n",
    "        \"\"\"\n",
    "        distance_reward = self.distance_reward(curr_state, next_state)\n",
    "        centering_reward = self.centering_reward(next_state[0], next_state[1])\n",
    "        # print(f\"Distance reward: {2*distance_reward}, Centering reward: {centering_reward}\")\n",
    "        return 2*distance_reward + centering_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexSelector:\n",
    "    def __init__(self, num_indices):\n",
    "        \"\"\"\n",
    "        Initialize the IndexSelector class.\n",
    "\n",
    "        Parameters:\n",
    "        num_indices (int): Number of indices.\n",
    "        \"\"\"\n",
    "        self.num_indices = num_indices\n",
    "        self.visited_indices = set()\n",
    "        self.probabilities = np.ones(num_indices) / num_indices\n",
    "\n",
    "    def select_index(self):\n",
    "        \"\"\"\n",
    "        Select an index based on the current probabilities.\n",
    "\n",
    "        Returns:\n",
    "        int: Selected index.\n",
    "        \"\"\"\n",
    "        if len(self.visited_indices) == self.num_indices:\n",
    "            # Reset the probabilities and visited indices\n",
    "            print('Visited all indices, resetting')\n",
    "            self.visited_indices = set()\n",
    "            self.probabilities = np.ones(self.num_indices) / self.num_indices\n",
    "\n",
    "        # Select an index based on the current probabilities\n",
    "        random_idx = np.random.choice(np.arange(self.num_indices), p=self.probabilities)\n",
    "\n",
    "        # Update the probabilities\n",
    "        self.visited_indices.add(random_idx)\n",
    "        if len(self.visited_indices) < self.num_indices:\n",
    "            self.probabilities[random_idx] = 0\n",
    "            remaining_prob = 1 - np.sum(self.probabilities)\n",
    "            self.probabilities[self.probabilities > 0] += remaining_prob / np.sum(self.probabilities > 0)\n",
    "\n",
    "        return random_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Tenth_navigation:\n",
    "    \"\"\"\n",
    "    A class to navigate the F1Tenth environment using SARSA(λ) and Expected SARSA algorithms.\n",
    "\n",
    "    Attributes:\n",
    "        env (gym.Env): The gym environment.\n",
    "        sx, sy, stheta (float): Starting x, y coordinates and orientation.\n",
    "        save_path (str): Path to save the weights.\n",
    "        track_name (str): Name of the track.\n",
    "        num_agents (int): Number of agents.\n",
    "        map_path (str): Path to the map.\n",
    "        map_ext (str): Extension of the map file.\n",
    "        map_centers (np.ndarray): Array of map center coordinates.\n",
    "        index_selector (IndexSelector): Index selector for resetting positions.\n",
    "        random_seed (int): Random seed for reproducibility.\n",
    "        num_beams (int): Number of LiDAR beams.\n",
    "        n_features (int): Number of binary features.\n",
    "        binary_powers (np.ndarray): Powers of 2 for binary conversion.\n",
    "        angle (int): Angle for LiDAR.\n",
    "        num_states (int): Number of states.\n",
    "        num_actions (int): Number of actions.\n",
    "        angles_deg (np.ndarray): Array of angles in degrees.\n",
    "        angles_rad (np.ndarray): Array of angles in radians.\n",
    "        min_speed, max_speed (float): Minimum and maximum speed.\n",
    "        mu, sig (float): Mean and standard deviation for Gaussian speed distribution.\n",
    "        action_space (dict): Action space mapping.\n",
    "        weights (np.ndarray): Q-table weights.\n",
    "        ET (np.ndarray): Eligibility trace.\n",
    "        IS (np.ndarray): Instructive signal.\n",
    "        discount_factor (float): Discount factor for SARSA.\n",
    "        learning_rate (float): Learning rate for SARSA.\n",
    "        decay_rate (float): Decay rate for eligibility trace.\n",
    "        IS_decay_rate (float): Decay rate for instructive signal.\n",
    "        epsilon_treshold (float): Epsilon threshold for epsilon-greedy policy.\n",
    "        numerical_stability (float): Numerical stability constant.\n",
    "        reward_class (Reward): Reward calculation class.\n",
    "        reward, cumulative_reward (float): Reward values.\n",
    "        episode_reward (list): List of episode rewards.\n",
    "        curr_state (int): Current state index.\n",
    "        curr_action_index (int): Current action index.\n",
    "        collision_count (int): Collision count.\n",
    "        distance_threshold (float): Distance threshold for reset.\n",
    "        state_counter (np.ndarray): Counter for state visits.\n",
    "        reset_position (np.ndarray): Reset position coordinates.\n",
    "        current_lap (int): Current lap count.\n",
    "        visited_states (set): Set of visited states.\n",
    "        visited_indices (set): Set of visited indices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gym_env_code='f110_gym:f110-v0', num_agents=1, map_path='./f1tenth_racetracks/Austin/Austin_map', map_ext='.png', sx=0., sy=0., stheta=0., map_centers_file=None, save_path=None, track_name=None, inference=None):\n",
    "        \"\"\"\n",
    "        Initialize the F1Tenth_navigation class.\n",
    "\n",
    "        Parameters:\n",
    "            gym_env_code (str): Gym environment code.\n",
    "            num_agents (int): Number of agents.\n",
    "            map_path (str): Path to the map.\n",
    "            map_ext (str): Extension of the map file.\n",
    "            sx, sy, stheta (float): Starting x, y coordinates and orientation.\n",
    "            map_centers_file (str): Path to the map centers file.\n",
    "            save_path (str): Path to save the weights.\n",
    "            track_name (str): Name of the track.\n",
    "            inference (str): Path to the inference weights file.\n",
    "        \"\"\"\n",
    "        self.env = gym.make(gym_env_code, map=map_path, map_ext=map_ext, num_agents=num_agents, timestep=0.01, integrator=Integrator.RK4)\n",
    "        self.env.add_render_callback(self.render_callback)\n",
    "        self.sx, self.sy, self.stheta = sx, sy, stheta\n",
    "        self.save_path = save_path\n",
    "        self.track_name = track_name\n",
    "        self.num_agents = num_agents\n",
    "        self.map_path = map_path\n",
    "        self.map_ext = map_ext\n",
    "        self.map_centers_file = pd.read_csv(map_centers_file)\n",
    "        self.map_centers_file.columns = ['x', 'y', 'w_r', 'w_l']\n",
    "        self.map_centers_file.index = self.map_centers_file.index.astype(int)\n",
    "        self.map_centers = self.map_centers_file.values[:, :2]\n",
    "\n",
    "        self.index_selector = IndexSelector(self.map_centers.shape[0])\n",
    "\n",
    "        self.random_seed = 42\n",
    "        np.random.seed(self.random_seed)\n",
    "\n",
    "        self.num_beams = 1080\n",
    "        self.n_features = 10\n",
    "        self.binary_powers = np.array([2 ** i for i in range(self.n_features)])\n",
    "        self.angle = 270\n",
    "\n",
    "        self.num_states = 2 ** self.n_features\n",
    "        self.num_actions = 51\n",
    "\n",
    "        self.angles_deg = np.linspace(-self.angle // 2, self.angle // 2, self.num_actions)[::-1]\n",
    "        self.angles_rad = self.convert_deg_to_rad(self.angles_deg)\n",
    "\n",
    "        self.min_speed = 0.7\n",
    "        self.max_speed = 2\n",
    "        self.mu = self.num_actions // 2\n",
    "        self.sig = self.num_actions // 4\n",
    "\n",
    "        self.action_space = self.generate_action_space(self.angles_rad)\n",
    "\n",
    "        if inference is not None:\n",
    "            self.weights = np.load(inference)\n",
    "            print(f'Loaded already trained weights')\n",
    "            self.collision_count = int(inference.split('_')[1].split('.')[0])\n",
    "        else:\n",
    "            self.weights = np.zeros((self.num_states, self.num_actions))\n",
    "            self.collision_count = 0\n",
    "        \n",
    "        self.ET_IS = np.zeros((self.num_states, self.num_actions))\n",
    "\n",
    "        # self.ET = np.zeros((self.num_states, 1))\n",
    "        # self.IS = np.zeros((1, self.num_actions))\n",
    "\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01\n",
    "        self.decay_rate = 0.9\n",
    "        self.IS_decay_rate = 0.7\n",
    "        self.epsilon_decay_rate=0.99997\n",
    "\n",
    "        self.epsilon_treshold = 0.05 #0.2\n",
    "        self.numerical_stability = 1e-5\n",
    "\n",
    "        self.reward_class = Reward(min_speed=self.min_speed, max_speed=self.max_speed, map_centers=self.map_centers)\n",
    "        self.reward = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.episode_reward = []\n",
    "\n",
    "        self.curr_state = None\n",
    "        self.curr_action_index = None\n",
    "\n",
    "        self.distance_threshold = 0.5\n",
    "        self.state_counter = np.zeros(self.num_states)\n",
    "        self.reset_position = np.array([[self.sx, self.sy]])\n",
    "\n",
    "        self.current_lap = 1\n",
    "        self.visited_states = set()\n",
    "        self.visited_indices = set()\n",
    "\n",
    "    def render_callback(self, env_renderer):\n",
    "        \"\"\"\n",
    "        Render callback function to update the camera to follow the car.\n",
    "\n",
    "        Parameters:\n",
    "            env_renderer (EnvRenderer): The environment renderer.\n",
    "        \"\"\"\n",
    "        e = env_renderer\n",
    "        x = e.cars[0].vertices[::2]\n",
    "        y = e.cars[0].vertices[1::2]\n",
    "        top, bottom, left, right = max(y), min(y), min(x), max(x)\n",
    "        e.score_label.x = left\n",
    "        e.score_label.y = top - 700\n",
    "        e.left = left - 800\n",
    "        e.right = right + 800\n",
    "        e.top = top + 800\n",
    "        e.bottom = bottom - 800\n",
    "\n",
    "    def convert_deg_to_rad(self, array):\n",
    "        \"\"\"\n",
    "        Convert an array of angles from degrees to radians.\n",
    "\n",
    "        Parameters:\n",
    "            array (np.ndarray): Array of angles in degrees.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of angles in radians.\n",
    "        \"\"\"\n",
    "        return array * np.pi / 180\n",
    "\n",
    "    def gaussian_speed(self, location):\n",
    "        \"\"\"\n",
    "        Calculate the Gaussian speed for a given location.\n",
    "\n",
    "        Parameters:\n",
    "            location (int): Location index.\n",
    "\n",
    "        Returns:\n",
    "            float: Gaussian speed.\n",
    "        \"\"\"\n",
    "        speed = (1 / (self.sig * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((location - self.mu) / self.sig) ** 2)\n",
    "        scale_factor = self.max_speed / (1 / (self.sig * np.sqrt(2 * np.pi)))\n",
    "        scaled_value = speed * scale_factor\n",
    "        return np.round(np.clip(scaled_value, self.min_speed, self.max_speed), 4)\n",
    "\n",
    "    def generate_action_space(self, angles):\n",
    "        \"\"\"\n",
    "        Generate the action space mapping.\n",
    "\n",
    "        Parameters:\n",
    "            angles (np.ndarray): Array of angles in radians.\n",
    "\n",
    "        Returns:\n",
    "            dict: Action space mapping.\n",
    "        \"\"\"\n",
    "        action_space = {}\n",
    "        for idx, angle in enumerate(angles):\n",
    "            action_space[idx] = (angle, self.gaussian_speed(idx))\n",
    "        return action_space\n",
    "\n",
    "    def lidar_to_binary_features(self, lidar_input, n_features=10, n_sectors=36, threshold_percentile=50):\n",
    "        \"\"\"\n",
    "        Convert LiDAR scan of 1080 values to a binary representation of 10 bits.\n",
    "\n",
    "        Parameters:\n",
    "            lidar_input (np.ndarray): LiDAR input array of shape (1080,).\n",
    "            n_features (int): Number of binary features to generate (default: 10).\n",
    "            n_sectors (int): Number of sectors to divide scan into (default: 36).\n",
    "            threshold_percentile (int): Percentile for thresholding (default: 50).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Binary features array of shape (n_features,).\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(self.random_seed)\n",
    "        lidar_input = np.asarray(lidar_input, dtype=np.float32)\n",
    "\n",
    "        sector_size = lidar_input.shape[0] // n_sectors\n",
    "        sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "\n",
    "        sector_features = np.concatenate([\n",
    "            np.mean(sectors, axis=1),\n",
    "            np.std(sectors, axis=1),\n",
    "            np.max(sectors, axis=1),\n",
    "            np.min(sectors, axis=1),\n",
    "            np.median(sectors, axis=1),\n",
    "            np.percentile(sectors, 75, axis=1)\n",
    "        ])\n",
    "\n",
    "        gradients = []\n",
    "        for scale in [1, 2, 3]:\n",
    "            rolled = np.roll(sector_features, scale)\n",
    "            gradient = sector_features - rolled\n",
    "            gradients.append(gradient)\n",
    "        gradient_features = np.concatenate(gradients)\n",
    "\n",
    "        combined_features = np.concatenate([sector_features, gradient_features, np.abs(gradient_features)])\n",
    "        combined_features = (combined_features - np.mean(combined_features)) / (np.std(combined_features) + 1e-8)\n",
    "\n",
    "        input_dim = combined_features.shape[0]\n",
    "        projection_matrix = rng.normal(0, 1, (input_dim, n_features))\n",
    "        projection_matrix /= np.linalg.norm(projection_matrix, axis=0) + 1e-8\n",
    "\n",
    "        projected_features = np.dot(combined_features, projection_matrix)\n",
    "        thresholds = np.percentile(projected_features, threshold_percentile, axis=0)\n",
    "\n",
    "        temperature = 0.7\n",
    "        z_scores = (projected_features - thresholds) / temperature\n",
    "        probabilities = stats.norm.cdf(z_scores)\n",
    "\n",
    "        random_matrix = rng.uniform(0, 1, probabilities.shape)\n",
    "        binary_features = (probabilities >= random_matrix).astype(np.int8)\n",
    "\n",
    "        return binary_features\n",
    "\n",
    "    def get_state(self, feature):\n",
    "        \"\"\"\n",
    "        Get the state index from the binary feature vector.\n",
    "\n",
    "        Parameters:\n",
    "            feature (np.ndarray): Binary feature vector.\n",
    "\n",
    "        Returns:\n",
    "            int: State index.\n",
    "        \"\"\"\n",
    "        return np.dot(feature[0], self.binary_powers)\n",
    "\n",
    "    def select_action(self, state_idx):\n",
    "        \"\"\"\n",
    "        Select an action based on the epsilon-greedy policy.\n",
    "\n",
    "        Parameters:\n",
    "            state_idx (int): State index.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Action index and action (steering angle, speed).\n",
    "        \"\"\"\n",
    "        random_number = np.random.rand()\n",
    "        action_idx = None\n",
    "\n",
    "        if random_number <= self.epsilon_treshold:\n",
    "            action_idx = np.random.choice(list(self.action_space.keys()))\n",
    "        else:\n",
    "            max_value = np.max(self.weights[state_idx])\n",
    "            max_indices = np.where(np.abs(self.weights[state_idx] == max_value))[0]\n",
    "            action_idx = np.random.choice(max_indices)\n",
    "\n",
    "        self.epsilon_treshold = self.epsilon_decay_rate * self.epsilon_treshold\n",
    "        return action_idx, self.action_space[action_idx]\n",
    "\n",
    "    def inference_action(self, state_idx):\n",
    "        \"\"\"\n",
    "        Select the action with the highest Q-value for inference.\n",
    "\n",
    "        Parameters:\n",
    "            state_idx (int): State index.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Action index and action (steering angle, speed).\n",
    "        \"\"\"\n",
    "        action_idx = np.argmax(self.weights[state_idx])\n",
    "        return action_idx, self.action_space[action_idx]\n",
    "\n",
    "    def normalize_weights(self,values, neg_lower_bound=-3, neg_upper_bound=-0.1, pos_lower_bound=0.1, pos_upper_bound=3):\n",
    "        values = np.array(values)\n",
    "    \n",
    "        pos_mask = values > 0\n",
    "        neg_mask = values < 0\n",
    "        \n",
    "        pos_values = values[pos_mask]\n",
    "        neg_values = values[neg_mask]\n",
    "        \n",
    "        normalized_values = np.zeros_like(values, dtype=float)\n",
    "        \n",
    "        if pos_values.size > 0:\n",
    "            pos_min = pos_values.min()\n",
    "            pos_max = pos_values.max()\n",
    "            if pos_min != pos_max:\n",
    "                pos_normalized = (pos_values - pos_min) / (pos_max - pos_min)\n",
    "                pos_scaled = pos_lower_bound + (pos_upper_bound - pos_lower_bound) * pos_normalized\n",
    "            else:\n",
    "                pos_scaled = np.full(pos_values.shape, pos_lower_bound)\n",
    "            normalized_values[pos_mask] = pos_scaled\n",
    "        \n",
    "        if neg_values.size > 0:\n",
    "            neg_min = neg_values.min()\n",
    "            neg_max = neg_values.max()\n",
    "            if neg_min != neg_max:\n",
    "                neg_normalized = (neg_values - neg_min) / (neg_max - neg_min)\n",
    "                neg_scaled = neg_lower_bound + (neg_upper_bound - neg_lower_bound) * neg_normalized\n",
    "            else:\n",
    "                neg_scaled = np.full(neg_values.shape, neg_upper_bound)\n",
    "            normalized_values[neg_mask] = neg_scaled\n",
    "        \n",
    "        return np.round(normalized_values,4)\n",
    "\n",
    "    def expected_sarsa_weight_update(self, next_state, reward):\n",
    "        \"\"\"\n",
    "        Update the weights using the Expected SARSA algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            next_state (int): Next state index.\n",
    "            reward (float): Reward received.\n",
    "        \"\"\"\n",
    "        # self.ET[self.curr_state, :] = 1\n",
    "        # self.IS[:, self.curr_action_index] = 1\n",
    "        self.ET_IS[self.curr_state, self.curr_action_index] = 1\n",
    "\n",
    "        q_values = self.weights[next_state, :]\n",
    "        q_max = np.max(q_values)\n",
    "        greedy_actions = np.sum(q_values == q_max)\n",
    "\n",
    "        non_greedy_action_probability = self.epsilon_treshold / self.num_actions\n",
    "        greedy_action_probability = ((1 - self.epsilon_treshold) / greedy_actions) + non_greedy_action_probability\n",
    "\n",
    "        greedy_mask = (q_values == q_max)\n",
    "        non_greedy_mask = ~greedy_mask\n",
    "\n",
    "        expected_q = np.sum(q_values[greedy_mask] * greedy_action_probability) + np.sum(q_values[non_greedy_mask] * non_greedy_action_probability)\n",
    "\n",
    "        delta = reward + self.discount_factor * expected_q - self.weights[self.curr_state, self.curr_action_index]\n",
    "\n",
    "        # self.weights += self.learning_rate * delta * (self.ET @ self.IS)\n",
    "        self.weights += self.learning_rate * delta * self.ET_IS\n",
    "        self.weights = self.normalize_weights(self.weights)\n",
    "        self.decay_ET_IS()\n",
    "\n",
    "    def sarsa_lambda_weight_update(self, next_state, reward):\n",
    "        \"\"\"\n",
    "        Update the weights using the SARSA(λ) algorithm.\n",
    "\n",
    "        Parameters:\n",
    "            next_state (int): Next state index.\n",
    "            reward (float): Reward received.\n",
    "        \"\"\"\n",
    "        # self.ET[self.curr_state, :] = 1\n",
    "        # self.IS[:, self.curr_action_index] = 1\n",
    "        self.ET_IS[self.curr_state, self.curr_action_index] = 1\n",
    "\n",
    "        next_action_index, _ = self.select_action(next_state)\n",
    "        delta = reward + self.discount_factor * self.weights[next_state, next_action_index] - self.weights[self.curr_state, self.curr_action_index]\n",
    "\n",
    "        # self.weights += self.learning_rate * delta * (self.ET @ self.IS)\n",
    "        self.weights += self.learning_rate * delta * self.ET_IS\n",
    "        self.weights = self.normalize_weights(self.weights)\n",
    "        self.decay_ET_IS()\n",
    "\n",
    "    def decay_ET_IS(self):\n",
    "        \"\"\"\n",
    "        Decay the eligibility trace and instructive signal.\n",
    "        \"\"\"\n",
    "        # self.ET *= self.discount_factor * self.decay_rate\n",
    "        # self.IS *= self.discount_factor * self.IS_decay_rate\n",
    "        self.ET_IS *= self.discount_factor * self.decay_rate\n",
    "\n",
    "        # self.ET = np.round(self.ET, 4)\n",
    "        # self.IS = np.round(self.IS, 4)\n",
    "        self.ET_IS = np.round(self.ET_IS, 4)\n",
    "\n",
    "    def reset_ET_IS(self):\n",
    "        \"\"\"\n",
    "        Reset the eligibility trace and instructive signal.\n",
    "        \"\"\"\n",
    "        # self.ET.fill(0)\n",
    "        # self.IS.fill(0)\n",
    "        self.ET_IS.fill(0)\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train the agent using the Expected SARSA algorithm.\n",
    "        \"\"\"\n",
    "        obs, step_reward, done, info = self.env.reset(np.array([[self.sx, self.sy, self.stheta]]))\n",
    "        lidar = obs['scans'][0]\n",
    "        down_sampled = self.lidar_to_binary_features(lidar).reshape(1, -1)\n",
    "        self.curr_state = self.get_state(down_sampled)\n",
    "        episode_reward = 0\n",
    "        self.reset_position = np.array([[self.sx, self.sy]])\n",
    "        self.reward_class.reset(self.reset_position)\n",
    "        while True:\n",
    "            if self.curr_state not in self.visited_states:\n",
    "                self.visited_states.add(self.curr_state)\n",
    "\n",
    "            self.curr_action_index, (steering_angle, speed) = self.select_action(self.curr_state)\n",
    "            curr_x = obs['poses_x'][0]\n",
    "            curr_y = obs['poses_y'][0]\n",
    "            obs, reward, done, info = self.env.step(np.array([[steering_angle, speed]]))\n",
    "            lidar = obs['scans'][0]\n",
    "            down_sampled = self.lidar_to_binary_features(lidar).reshape(1, -1)\n",
    "            next_state = self.get_state(down_sampled)\n",
    "\n",
    "            if done:\n",
    "                self.reward = -100\n",
    "            else:\n",
    "                self.reward = self.reward_class.calculate_reward(np.array([curr_x, curr_y]), np.array([obs['poses_x'][0], obs['poses_y'][0]]))\n",
    "\n",
    "            self.cumulative_reward += self.reward\n",
    "            episode_reward += self.reward\n",
    "\n",
    "            self.expected_sarsa_weight_update(next_state, self.reward)\n",
    "            self.curr_state = next_state\n",
    "\n",
    "            if done:\n",
    "                random_idx = self.index_selector.select_index()\n",
    "                n_x, n_y = self.map_centers[random_idx]\n",
    "                delta_x, delta_y = np.random.uniform(-0.75, 0.75), np.random.uniform(-0.2, 0.2)\n",
    "                n_theta = np.random.choice(self.angles_rad)\n",
    "                obs, step_reward, done, info = self.env.reset(np.array([[n_x + delta_x, n_y + delta_y, n_theta]]))\n",
    "                lidar = obs['scans'][0]\n",
    "                down_sampled = self.lidar_to_binary_features(lidar).reshape(1, -1)\n",
    "                self.curr_state = self.get_state(down_sampled)\n",
    "\n",
    "                self.reset_position = np.array([[n_x + delta_x, n_y + delta_y]])\n",
    "                self.reward_class.reset(self.reset_position)\n",
    "                self.reset_ET_IS()\n",
    "                \n",
    "                if (self.collision_count + 1) % 1000 == 0:\n",
    "                    self.episode_reward.append(episode_reward)\n",
    "                    print(f'Episode:{self.collision_count + 1}, reward:{np.mean(self.episode_reward)}, Cumulative Reward:{self.cumulative_reward}, observed states:{len(self.visited_states)}')\n",
    "                    episode_reward = 0\n",
    "                    self.episode_reward = []\n",
    "                    \n",
    "                if (self.collision_count + 1) % 5000 == 0:\n",
    "                    if not os.path.exists(os.path.join(self.save_path, self.track_name)):\n",
    "                        os.mkdir(os.path.join(self.save_path, self.track_name))\n",
    "                    np.save(os.path.join(self.save_path, self.track_name, f'weights_{self.collision_count + 1}.npy'), self.weights)\n",
    "                    print(f'File saved')\n",
    "                self.collision_count += 1\n",
    "\n",
    "            # self.env.render(mode='human')\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Inference the agent using the Expected SARSA algorithm.\n",
    "        \"\"\"\n",
    "        obs, reward, done, info = self.env.reset(np.array([[self.sx, self.sy, self.stheta]]))\n",
    "        lidar = obs['scans'][0]\n",
    "        down_sampled = self.lidar_to_binary_features(lidar).reshape(1, -1)\n",
    "        self.curr_state = self.get_state(down_sampled)\n",
    "\n",
    "        while not done:\n",
    "            _, (steering_angle, speed) = self.inference_action(self.curr_state)\n",
    "            obs, reward, done, info = self.env.step(np.array([[steering_angle, speed]]))\n",
    "            lidar = obs['scans'][0]\n",
    "            down_sampled = self.lidar_to_binary_features(lidar).reshape(1, -1)\n",
    "            self.curr_state = self.get_state(down_sampled)       \n",
    "            np.save(f'lidar_{self.curr_state}.npy', lidar)     \n",
    "            if done:\n",
    "                random_idx = self.index_selector.select_index()\n",
    "                n_x, n_y = self.map_centers[random_idx]\n",
    "                delta_x, delta_y = np.random.uniform(-0.75, 0.75), np.random.uniform(-0.2, 0.2)\n",
    "                n_theta = np.random.choice(self.angles_rad)\n",
    "                obs, step_reward, done, info = self.env.reset(np.array([[n_x + delta_x, n_y + delta_y, n_theta]]))\n",
    "                lidar = obs['scans'][0]\n",
    "                down_sampled = self.lidar_to_binary_features(lidar).reshape(1, -1)\n",
    "                self.curr_state = self.get_state(down_sampled)\n",
    "                \n",
    "                self.reset_position = np.array([[n_x + delta_x, n_y + delta_y]])\n",
    "                self.reward_class.reset(self.reset_position)\n",
    "                self.reset_ET_IS()\n",
    "                self.collision_count+=1\n",
    "\n",
    "            self.env.render(mode='human')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/workspaces/F1_tenth_training/f1tenth_gym_ros/f1tenth_racetracks'\n",
    "path = './f1tenth_racetracks'\n",
    "all_map_paths=[]\n",
    "map_centers = []\n",
    "map_names = []\n",
    "track_lengths=[]\n",
    "for folder in os.listdir(path):\n",
    "    if folder not in ['README.md','.gitignore','convert.py','LICENSE','rename.py','.git']:\n",
    "        folder_name=folder\n",
    "        file_name=folder_name.replace(' ','')+'_map'\n",
    "        map_center = folder_name.replace(' ','')+'_centerline.csv'\n",
    "        track_lengths.append(len(pd.read_csv(f'{path}/{folder_name}/{map_center}')))\n",
    "        map_names.append(folder_name)\n",
    "        all_map_paths.append(f'{path}/{folder_name}/{file_name}')\n",
    "        map_centers.append(f'{path}/{folder_name}/{map_center}')\n",
    "\n",
    "list(zip(map_names,track_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global num_agents,map_path,map_ext,sx,sy,stheta\n",
    "num_agents = 1\n",
    "map_ext = '.png'\n",
    "sx = 0.\n",
    "sy = 0.\n",
    "stheta = 1\n",
    "map_path = all_map_paths[-8]\n",
    "map_center = map_centers[-8]\n",
    "map_name = map_names[-8]\n",
    "# save_path = '/workspaces/F1_tenth_training/f1tenth_gym_ros/Weights/'\n",
    "save_path = './Weights/'\n",
    "inference_file = None\n",
    "# inference_file = '/workspaces/F1_tenth_training/f1tenth_gym_ros/Weights/Melbourne/weights_15000.npy'\n",
    "# inference_file = './Weights/Montreal/weights_55000.npy'\n",
    "simulator = F1Tenth_navigation(num_agents=num_agents,map_path=map_path,map_ext=map_ext,sx=sx,sy=sy,stheta=stheta,map_centers_file=map_center,save_path=save_path,track_name=map_name,inference=inference_file)\n",
    "# simulator.train()\n",
    "simulator.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save(f'{save_path}{map_name}/weights_2000.npy',simulator.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(values, neg_lower_bound, neg_upper_bound, pos_lower_bound, pos_upper_bound):\n",
    "    values = np.array(values)\n",
    "    \n",
    "    pos_mask = values > 0\n",
    "    neg_mask = values < 0\n",
    "    \n",
    "    pos_values = values[pos_mask]\n",
    "    neg_values = values[neg_mask]\n",
    "    \n",
    "    normalized_values = np.zeros_like(values, dtype=float)\n",
    "    \n",
    "    if pos_values.size > 0:\n",
    "        pos_min = pos_values.min()\n",
    "        pos_max = pos_values.max()\n",
    "        if pos_min != pos_max:\n",
    "            pos_normalized = (pos_values - pos_min) / (pos_max - pos_min)\n",
    "            pos_scaled = pos_lower_bound + (pos_upper_bound - pos_lower_bound) * pos_normalized\n",
    "        else:\n",
    "            pos_scaled = np.full(pos_values.shape, pos_lower_bound)\n",
    "        normalized_values[pos_mask] = pos_scaled\n",
    "    \n",
    "    if neg_values.size > 0:\n",
    "        neg_min = neg_values.min()\n",
    "        neg_max = neg_values.max()\n",
    "        if neg_min != neg_max:\n",
    "            neg_normalized = (neg_values - neg_min) / (neg_max - neg_min)\n",
    "            neg_scaled = neg_lower_bound + (neg_upper_bound - neg_lower_bound) * neg_normalized\n",
    "        else:\n",
    "            neg_scaled = np.full(neg_values.shape, neg_upper_bound)\n",
    "        normalized_values[neg_mask] = neg_scaled\n",
    "    \n",
    "    return np.round(normalized_values,4)\n",
    "\n",
    "# Example usage\n",
    "values = [-1, -20, -30, -40, -50, 15, -11, -2, -3, -4, -5, -6, -7, -8, -9, -10,-60]\n",
    "values = [1,1,1,-0.99,-0.78,1,1,1,2,1,87,0.8,0.02,1]\n",
    "values = [1]*4+[0.8,0.12,-0.97,0.87]\n",
    "neg_lower_bound = -3\n",
    "neg_upper_bound = -0.1\n",
    "pos_lower_bound = 0.1\n",
    "pos_upper_bound = 3\n",
    "normalized_values = normalize(values, neg_lower_bound, neg_upper_bound, pos_lower_bound, pos_upper_bound)\n",
    "print(normalized_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def normalize_weights(weights, new_max=3, new_min=-3):\n",
    "        \"\"\"\n",
    "        Normalize the weights to be within the range [-3, 3].\n",
    "\n",
    "        Parameters:\n",
    "            weights (np.ndarray): The array of weights to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The normalized weights.\n",
    "        \"\"\"\n",
    "        current_min = np.min(weights)\n",
    "        current_max = np.max(weights)\n",
    "\n",
    "        scaled = (weights - current_min) / (current_max - current_min) * (new_max - new_min) + new_min\n",
    "\n",
    "        return np.round(scaled, 4)\n",
    "normalize_weights(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = np.ones((1,10))\n",
    "print(z)\n",
    "z.fill(0)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_binary_representation(vector, num_bins=16, window_size=3, stride=1):\n",
    "    \"\"\"\n",
    "    Convert a vector into a binary representation while preserving similarity (vectorized).\n",
    "\n",
    "    Parameters:\n",
    "    - vector: 1D array of real values.\n",
    "    - num_bins: Number of bins for quantization (default: 10).\n",
    "    - window_size: Size of each sliding window (default: 3).\n",
    "    - stride: Step size for the sliding window (default: 2).\n",
    "\n",
    "    Returns:\n",
    "    - binary_code: 1D array (1 x num_bins) representing the vector.\n",
    "    \"\"\"\n",
    "    # Generate sliding windows using stride\n",
    "    num_windows = (vector.shape[0] - window_size) // stride + 1\n",
    "    indices = np.arange(window_size)[None, :] + stride * np.arange(num_windows)[:, None]\n",
    "    windows = vector[indices]  # Shape: (num_windows, window_size)\n",
    "\n",
    "    # Aggregate each window (mean aggregation)\n",
    "    aggregated = windows.mean(axis=1)  # Shape: (num_windows,)\n",
    "\n",
    "    # Normalize aggregated values to [0, 1]\n",
    "    min_val, max_val = aggregated.min(), aggregated.max()\n",
    "    normalized = (aggregated - min_val) / (max_val - min_val)\n",
    "\n",
    "    # Quantize to bins\n",
    "    quantized = np.floor(normalized * num_bins).astype(int)\n",
    "    quantized = np.clip(quantized, 0, num_bins - 1)  # Ensure indices are in range\n",
    "\n",
    "    # Create one-hot encoding for each quantized value\n",
    "    onehot_codes = np.zeros((len(quantized), num_bins), dtype=int)\n",
    "    onehot_codes[np.arange(len(quantized)), quantized] = 1\n",
    "\n",
    "    # Combine all one-hot vectors into a single binary vector\n",
    "    binary_code = (onehot_codes.sum(axis=0) > 0).astype(int)  # Binary OR across rows\n",
    "    return np.array(binary_code).reshape(1,-1)\n",
    "\n",
    "def lidar_to_binary(lidar_input, n_sectors=30, num_bins=16, window_size=3, stride=1):\n",
    "    \"\"\"\n",
    "    Convert LiDAR input into a combined binary representation using statistical features.\n",
    "\n",
    "    Parameters:\n",
    "    - lidar_input: 1D array of LiDAR values.\n",
    "    - n_sectors: Number of sectors to divide the input into.\n",
    "    - num_bins: Number of bins for quantization.\n",
    "    - window_size: Size of the sliding window for binary encoding.\n",
    "    - stride: Step size for the sliding window.\n",
    "\n",
    "    Returns:\n",
    "    - combined_binary: 1D array of binary representation.\n",
    "    \"\"\"\n",
    "    # Ensure input is a NumPy array\n",
    "    lidar_input = np.asarray(lidar_input, dtype=np.float32)\n",
    "    \n",
    "    # Divide LiDAR input into sectors\n",
    "    sector_size = lidar_input.shape[0] // n_sectors\n",
    "    sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "    \n",
    "    # Calculate statistical features\n",
    "    sector_features = np.vstack(\n",
    "        [\n",
    "            np.mean(sectors, axis=1),\n",
    "            np.std(sectors, axis=1),\n",
    "            np.max(sectors, axis=1),\n",
    "            np.min(sectors, axis=1),\n",
    "            np.median(sectors, axis=1),\n",
    "            np.percentile(sectors, 75, axis=1),\n",
    "        ]\n",
    "    )\n",
    "    # sector_features = np.vstack([\n",
    "    #     vectorized_binary_representation(np.mean(sectors, axis=1), num_bins, window_size, stride),\n",
    "    #     vectorized_binary_representation(np.std(sectors, axis=1), num_bins, window_size, stride),\n",
    "    #     vectorized_binary_representation(np.max(sectors, axis=1), num_bins, window_size, stride),\n",
    "    #     vectorized_binary_representation(np.min(sectors, axis=1), num_bins, window_size, stride),\n",
    "    #     vectorized_binary_representation(np.median(sectors, axis=1), num_bins, window_size, stride),\n",
    "    #     vectorized_binary_representation(np.percentile(sectors, 75,axis=1), num_bins, window_size, stride),\n",
    "    # ])\n",
    "    \n",
    "    return sector_features\n",
    "\n",
    "    binary_code = (sector_features.sum(axis=0) > 0).astype(int)  # Binary OR across rows\n",
    "    return np.array(binary_code).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def consolidate_matrix(matrix, n_pca_components=3, num_bins=12):\n",
    "    \"\"\"\n",
    "    Consolidate a (6, 30) matrix into a unique representation.\n",
    "    \n",
    "    Parameters:\n",
    "    - matrix: 2D array of shape (6, 30).\n",
    "    - n_pca_components: Number of PCA components to retain.\n",
    "    - num_bins: Number of bins for quantization.\n",
    "\n",
    "    Returns:\n",
    "    - consolidated_representation: 1D array representing the matrix.\n",
    "    \"\"\"\n",
    "    # Step 1: Compute row-wise statistical features\n",
    "    row_features = np.hstack([\n",
    "        np.mean(matrix, axis=1),\n",
    "        np.std(matrix, axis=1),\n",
    "        np.min(matrix, axis=1),\n",
    "        np.max(matrix, axis=1),\n",
    "        np.median(matrix, axis=1)\n",
    "    ])  # Shape: (6 * 5,)\n",
    "\n",
    "    # Step 2: Apply PCA to capture inter-row correlations\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    pca_features = pca.fit_transform(matrix).flatten()  # Shape: (n_pca_components,)\n",
    "\n",
    "    # Step 3: Quantize row_features into binary format (optional)\n",
    "    def quantize_to_binary(features, num_bins):\n",
    "        min_val, max_val = features.min(), features.max()\n",
    "        normalized = (features - min_val) / (max_val - min_val)\n",
    "        quantized = np.floor(normalized * num_bins).astype(int)\n",
    "        binary = np.unpackbits(\n",
    "            quantized.astype(np.uint8).reshape(-1, 1), axis=1\n",
    "        ).flatten()\n",
    "        return binary\n",
    "\n",
    "    binary_features = quantize_to_binary(row_features, num_bins)\n",
    "\n",
    "    # Step 4: Combine row features, PCA features, and binary encoding\n",
    "    consolidated_representation = np.concatenate([row_features, pca_features, binary_features])\n",
    "    return consolidated_representation\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example matrix of shape (6, 30)\n",
    "    matrix = np.random.uniform(0, 10, size=(6, 30))\n",
    "    \n",
    "    # Generate consolidated representation\n",
    "    representation = consolidate_matrix(matrix, n_pca_components=6, num_bins=12)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Matrix Shape:\", matrix.shape)\n",
    "    print(\"Consolidated Representation Shape:\", representation.shape)\n",
    "    print(\"Representation:\", representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_1 = np.load('./lidar_115.npy')\n",
    "lidar_2 = np.load('./lidar_211.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_rows(matrix):\n",
    "    \"\"\"\n",
    "    Normalize each row of the matrix to the range [-1, 1].\n",
    "    \n",
    "    Parameters:\n",
    "    matrix (np.ndarray): The input matrix to be normalized.\n",
    "    \n",
    "    Returns:\n",
    "    np.ndarray: The row-normalized matrix.\n",
    "    \"\"\"\n",
    "    # Calculate the min and max for each row\n",
    "    row_min = matrix.min(axis=1, keepdims=True)\n",
    "    row_max = matrix.max(axis=1, keepdims=True)\n",
    "    \n",
    "    row_range = row_max - row_min\n",
    "    row_range[row_range < 1e-6] = 1.0\n",
    "    \n",
    "    # Scale to [-1, 1]\n",
    "    normalized = -1 + 2 * (matrix - row_min) / row_range\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lidar_to_binary_features(lidar_input, scaler,binary_threshold=0.0,n_sectors=12):\n",
    "    \"\"\"\n",
    "    Convert LiDAR input into a binary representation using statistical features.\n",
    "\n",
    "    Parameters:\n",
    "    - lidar_input: 1D array of LiDAR values.\n",
    "    - n_sectors: Number of sectors to divide the input into.\n",
    "\n",
    "    Returns:\n",
    "    - binary_code: 1D array of binary representation.\n",
    "    \"\"\"\n",
    "    lidar_input = np.asarray(lidar_input, dtype=np.float32)\n",
    "    sector_size = lidar_input.shape[0] // n_sectors\n",
    "    sectors = lidar_input[:sector_size * n_sectors].reshape(n_sectors, sector_size)\n",
    "    sector_features = np.vstack(\n",
    "        [   np.mean(sectors, axis=1),\n",
    "            np.std(sectors, axis=1),\n",
    "            np.max(sectors, axis=1),\n",
    "            np.min(sectors, axis=1),\n",
    "            np.median(sectors, axis=1),\n",
    "            np.percentile(sectors, 75, axis=1)\n",
    "        ])\n",
    "    print(sector_features)\n",
    "    scaled_features = scaler.partial_fit(sector_features.T).transform(sector_features.T).T\n",
    "    # scaled_features = normalize_rows(sector_features)\n",
    "    print(scaled_features)\n",
    "    binary_features = (scaled_features > binary_threshold).astype(int)\n",
    "\n",
    "    binary_code = (binary_features.sum(axis=0) > -0).astype(int)  \n",
    "    return np.array(binary_code).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.54263496e+00 8.76836479e-01 6.92538857e-01 6.48725688e-01\n",
      "  7.10616350e-01 9.69677627e-01 2.04411626e+00 1.09461365e+01\n",
      "  2.12103939e+00 1.18191600e+00 9.19106364e-01 8.76223862e-01]\n",
      " [3.83783132e-01 8.76355916e-02 3.22521329e-02 2.29667313e-02\n",
      "  4.84589860e-02 1.15076408e-01 7.21159697e-01 8.14948750e+00\n",
      "  5.43388963e-01 1.26159295e-01 6.16060272e-02 2.84680519e-02]\n",
      " [2.42389607e+00 1.11615288e+00 7.35619664e-01 7.27994204e-01\n",
      "  7.98289120e-01 1.21846759e+00 3.81646538e+00 2.83156891e+01\n",
      "  3.31884933e+00 1.44437301e+00 1.04671645e+00 9.45689917e-01]\n",
      " [1.08665156e+00 6.99398935e-01 6.17003798e-01 6.17931604e-01\n",
      "  6.21799469e-01 8.34172428e-01 1.21807468e+00 3.52457047e+00\n",
      "  1.41544938e+00 1.01784217e+00 8.42545152e-01 8.24438035e-01]\n",
      " [1.42553592e+00 8.74777257e-01 7.08054006e-01 6.43826723e-01\n",
      "  7.12303400e-01 9.23525631e-01 1.89916515e+00 7.16892815e+00\n",
      "  2.00764561e+00 1.16354322e+00 9.13262188e-01 8.68300140e-01]\n",
      " [1.77300334e+00 9.20066461e-01 7.15641052e-01 6.52724192e-01\n",
      "  7.25296497e-01 1.10809991e+00 2.56195217e+00 1.39925518e+01\n",
      "  2.55442560e+00 1.22646001e+00 9.84819666e-01 8.84077817e-01]]\n",
      "[[-1.51965200e-01 -3.93924307e-01 -4.60900260e-01 -4.76822488e-01\n",
      "  -4.54330684e-01 -3.60184727e-01  3.02790813e-02  3.26537936e+00\n",
      "   5.82338645e-02 -2.83054773e-01 -3.78562927e-01 -3.94146940e-01]\n",
      " [-2.15615387e-01 -3.49690904e-01 -3.74764777e-01 -3.78968577e-01\n",
      "  -3.67427413e-01 -3.37267564e-01 -6.28741657e-02  3.30016874e+00\n",
      "  -1.43356694e-01 -3.32249984e-01 -3.61475325e-01 -3.76477952e-01]\n",
      " [-1.88146904e-01 -3.63671251e-01 -4.14746145e-01 -4.15769629e-01\n",
      "  -4.06334696e-01 -3.49938645e-01 -1.23725762e-03  3.28703086e+00\n",
      "  -6.80269249e-02 -3.19617782e-01 -3.72990959e-01 -3.86550667e-01]\n",
      " [-3.04151031e-02 -5.35094417e-01 -6.42474245e-01 -6.41265101e-01\n",
      "  -6.36224383e-01 -3.59453533e-01  1.40859468e-01  3.14675439e+00\n",
      "   3.98084136e-01 -1.20089577e-01 -3.48541947e-01 -3.72139687e-01]\n",
      " [-1.05998667e-01 -4.24081044e-01 -5.20369550e-01 -5.57463051e-01\n",
      "  -5.17915376e-01 -3.95927154e-01  1.67538773e-01  3.21101110e+00\n",
      "   2.30190027e-01 -2.57308605e-01 -4.01854651e-01 -4.27821806e-01]\n",
      " [-1.59225904e-01 -3.98079343e-01 -4.55325914e-01 -4.72944930e-01\n",
      "  -4.52622037e-01 -3.45423119e-01  6.17085480e-02  3.26269340e+00\n",
      "   5.96008320e-02 -3.12277973e-01 -3.79946084e-01 -4.08157476e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=StandardScaler()\n",
    "bin_1 = lidar_to_binary_features(lidar_1,scaler,binary_threshold=0.0)\n",
    "# bin_2 = lidar_to_binary_features(lidar_2,scaler,binary_threshold=0.0)\n",
    "bin_1#, bin_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.418569  , 2.42389605, 2.30872974, ..., 0.92087105, 0.9323106 ,\n",
       "       0.94568994])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
